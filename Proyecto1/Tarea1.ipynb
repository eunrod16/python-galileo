{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hilda Eunice Rodas Rivas\n",
    "### 19000626\n",
    "### Tarea 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('proyecto_training_data.npy')\n",
    "x = dataset\n",
    "numrows = len(x)-round(len(x) * 0.2)\n",
    "datos_entrenamiento = x[:numrows, ]\n",
    "datos_entrenamiento = np.array(datos_entrenamiento)\n",
    "datos_entrenamiento = np.nan_to_num(datos_entrenamiento)\n",
    "x= datos_entrenamiento[:,1]\n",
    "y= datos_entrenamiento[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_epochs = 1000\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope('data_training'):\n",
    "    X = tf.placeholder(tf.float32 , name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32 , name = \"Y\")\n",
    "    learning_rate = tf.placeholder(tf.float32, [] ,name='learning_rate')\n",
    "with tf.name_scope('parameters'):\n",
    "    w = tf.Variable(0.0,name='W')\n",
    "    b = tf.Variable(0.0,name='B')\n",
    "\n",
    "with tf.name_scope('regression_model'):\n",
    "    Y_predicted = X*w + b\n",
    "with tf.name_scope('cost_function'):\n",
    "    loss = tf.reduce_mean(tf.square(Y-Y_predicted,name = 'cost'))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "cost = tf.summary.scalar('cost',loss)\n",
    "merged_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  0.01\n",
      "Epoch:  0 cost is:  4208969200.0\n",
      "Epoch:  50 cost is:  2743875600.0\n",
      "Epoch:  100 cost is:  2703196700.0\n",
      "Epoch:  150 cost is:  2666194000.0\n",
      "Epoch:  200 cost is:  2632536300.0\n",
      "Epoch:  250 cost is:  2601920300.0\n",
      "Epoch:  300 cost is:  2574072800.0\n",
      "Epoch:  350 cost is:  2548741400.0\n",
      "Epoch:  400 cost is:  2525699600.0\n",
      "Epoch:  450 cost is:  2504741400.0\n",
      "Epoch:  500 cost is:  2485677300.0\n",
      "Epoch:  550 cost is:  2468336400.0\n",
      "Epoch:  600 cost is:  2452563200.0\n",
      "Epoch:  650 cost is:  2438215200.0\n",
      "Epoch:  700 cost is:  2425164500.0\n",
      "Epoch:  750 cost is:  2413293600.0\n",
      "Epoch:  800 cost is:  2402495500.0\n",
      "Epoch:  850 cost is:  2392673000.0\n",
      "Epoch:  900 cost is:  2383739100.0\n",
      "Epoch:  950 cost is:  2375612200.0\n",
      "learning_rate:  0.001\n",
      "Epoch:  0 cost is:  33275544000.0\n",
      "Epoch:  50 cost is:  2791949300.0\n",
      "Epoch:  100 cost is:  2780145700.0\n",
      "Epoch:  150 cost is:  2775560000.0\n",
      "Epoch:  200 cost is:  2771019500.0\n",
      "Epoch:  250 cost is:  2766521900.0\n",
      "Epoch:  300 cost is:  2762067000.0\n",
      "Epoch:  350 cost is:  2757654000.0\n",
      "Epoch:  400 cost is:  2753282800.0\n",
      "Epoch:  450 cost is:  2748952000.0\n",
      "Epoch:  500 cost is:  2744662000.0\n",
      "Epoch:  550 cost is:  2740413000.0\n",
      "Epoch:  600 cost is:  2736204000.0\n",
      "Epoch:  650 cost is:  2732034800.0\n",
      "Epoch:  700 cost is:  2727904300.0\n",
      "Epoch:  750 cost is:  2723813000.0\n",
      "Epoch:  800 cost is:  2719760600.0\n",
      "Epoch:  850 cost is:  2715746300.0\n",
      "Epoch:  900 cost is:  2711769600.0\n",
      "Epoch:  950 cost is:  2707830000.0\n",
      "learning_rate:  0.0001\n",
      "Epoch:  0 cost is:  38244225000.0\n",
      "Epoch:  50 cost is:  18645508000.0\n",
      "Epoch:  100 cost is:  9880097000.0\n",
      "Epoch:  150 cost is:  5959670300.0\n",
      "Epoch:  200 cost is:  4206068200.0\n",
      "Epoch:  250 cost is:  3421546500.0\n",
      "Epoch:  300 cost is:  3070424800.0\n",
      "Epoch:  350 cost is:  2913134300.0\n",
      "Epoch:  400 cost is:  2842530600.0\n",
      "Epoch:  450 cost is:  2810697700.0\n",
      "Epoch:  500 cost is:  2796204300.0\n",
      "Epoch:  550 cost is:  2789466400.0\n",
      "Epoch:  600 cost is:  2786196200.0\n",
      "Epoch:  650 cost is:  2784476700.0\n",
      "Epoch:  700 cost is:  2783453000.0\n",
      "Epoch:  750 cost is:  2782738700.0\n",
      "Epoch:  800 cost is:  2782164000.0\n",
      "Epoch:  850 cost is:  2781651200.0\n",
      "Epoch:  900 cost is:  2781167400.0\n",
      "Epoch:  950 cost is:  2780696600.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.summary.FileWriter('/home/eunice/PAPD/python/Proyecto',sess.graph)\n",
    "    rates = [0.01,0.001,0.0001]\n",
    "    for rate in rates:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('learning_rate: ',rate)\n",
    "        for i in range(training_epochs):\n",
    "            _=sess.run(train_op,feed_dict={X:x,Y:y,learning_rate:rate})\n",
    "            if i%50==0:\n",
    "                cost_,summary=sess.run([loss,merged_summaries],feed_dict={X:x,Y:y})\n",
    "                print ('Epoch: ',i,'cost is: ',cost_)\n",
    "                summary_writer.add_summary(summary,i)\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph: \n",
    "![alt text](graph_run=.png \"Graph\")\n",
    "![alt text](graph_run=1.png \"Graph1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar Learning Rate 0.0001:\n",
    "![alt text](cost1.png \"Cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "El learning rate controla la rapidez con que el modelo se adapta al problema. Los learning rate más pequeños requieren más iteraciones , dado que los cambios más pequeños se hacen en las ponderaciones de cada actualización,  los learning rate más grandes producen cambios rápidos y requieren menos iteraciones como se puede ver con el learning rate de 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
